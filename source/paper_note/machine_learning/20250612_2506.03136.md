# 250612_Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning

---
**论文信息**

- **标题**: Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning
- **arXiv ID**: 2506.03136
- **作者**: Authors:Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, Mengdi Wang
- **发表日期**: 2025-06-03T17:58:42+00:00
- **论文链接**: [2506.03136](https://arxiv.org/abs/2506.03136)
- **总结生成时间**: 2025-06-12 17:23:10

---

**一句话概要**  
CURE框架通过强化学习协同进化代码生成与单元测试能力，无需真实代码监督即可显著提升大语言模型的编程准确性与测试效率。

**主体**  
当前大语言模型在代码生成任务中面临的核心挑战在于代码质量与测试覆盖率的双重保障。传统方法依赖人工标注的真实代码作为监督信号，既限制了训练灵活性，又难以捕捉代码与测试用例间的动态反馈关系。作者提出的CURE框架创新性地将强化学习引入这一领域，通过设计专用奖励机制，使代码生成器与单元测试器在交互过程中相互促进——测试器从生成器的错误中学习改进测试策略，而生成器则根据测试反馈优化代码质量，形成良性循环。

该方法的关键突破在于完全摆脱了对真实代码的依赖，仅通过两者交互结果构建奖励信号。具体实现中，作者采用分层强化学习架构，其中代码生成模块基于Qwen2.5-Instruct模型优化，测试生成模块则通过动态采样策略捕捉关键错误模式。实验数据显示，优化后的ReasonFlux-Coder模型在代码生成准确率上提升5.3%，Best-of-N准确率提升9%，显著超越同规模竞品。更值得注意的是，测试生成效率达到64.8%，且模型展现出色的迁移能力，在下游任务如测试时扩展和自主编程中实现8.1%的性能提升。

**启示**  
这项研究为构建自监督的编程智能系统开辟了新路径，其协同进化机制可扩展至其他需要多重能力配合的AI任务，如数学推理与验证、多模态内容生成与评估等。