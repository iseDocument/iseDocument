# 250615_ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests

---
**论文信息**

- **标题**: ICPC-Eval: Probing the Frontiers of LLM Reasoning with Competitive Programming Contests
- **arXiv ID**: 2506.04894
- **作者**: Authors:Shiyi Xu, Yiwen Hu, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Ji-Rong Wen
- **发表日期**: 2025-06-05T11:20:37+00:00
- **论文链接**: [2506.04894](https://arxiv.org/abs/2506.04894)
- **总结生成时间**: 2025-06-15 19:24:22

---

**一句话概要**  
作者团队提出ICPC-Eval基准测试，通过国际大学生程序设计竞赛（ICPC）的真实题目评估大语言模型的复杂推理能力，并设计迭代修复指标Refine@K揭示模型在反馈驱动下的潜力。

**主体**  
当前大语言模型在代码生成任务上虽取得进展，但现有评测基准（如LiveCodeBench）难以模拟真实竞赛环境，且传统指标Pass@K无法衡量模型的反思优化能力。为此，研究团队从全球11场ICPC赛事中精选118道题目构建ICPC-Eval，其核心创新在于三重设计：首先，题目类型与难度分布严格对标实际竞赛，覆盖动态规划、图论等高阶算法；其次，开发自动化测试用例生成工具，支持本地高效验证代码正确性；最关键的是提出Refine@K指标，允许模型根据执行反馈进行多轮代码迭代优化，从而量化其动态推理能力。

实验结果显示，顶尖模型如DeepSeek-R1在单次生成中表现有限，但通过Refine@K机制可显著提升性能——这表明反馈循环能有效激发模型的上下文推理潜力。不过横向对比发现，即使最优模型仍落后于人类顶尖选手约30%的解题率，尤其在需要创造性算法设计的难题上差距明显。可视化分析进一步揭示，模型在时间/空间复杂度优化方面存在系统性短板。

**最后一句**  
这项研究不仅为评估AI的复杂推理能力树立了新标准，其反馈驱动的评测框架也为未来开发具有持续自我优化能力的智能系统提供了方法论启示。