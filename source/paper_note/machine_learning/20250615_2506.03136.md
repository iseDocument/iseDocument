# 250615_Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning

---
**论文信息**

- **标题**: Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning
- **arXiv ID**: 2506.03136
- **作者**: Authors:Yinjie Wang, Ling Yang, Ye Tian, Ke Shen, Mengdi Wang
- **发表日期**: 2025-06-03T17:58:42+00:00
- **论文链接**: [2506.03136](https://arxiv.org/abs/2506.03136)
- **总结生成时间**: 2025-06-15 19:24:22

---

**一句话概要**  
CURE框架通过强化学习协同进化代码生成与单元测试能力，在无监督条件下显著提升大语言模型的编程准确性与测试效率。

**主体**  
当前大语言模型在代码生成任务中面临的核心挑战在于代码质量与测试验证的割裂——传统方法依赖人工标注的"标准答案"监督训练，既限制了模型自我改进的空间，也难以实现代码生成与测试能力的动态适配。作者提出的CURE框架创新性地将强化学习机制引入这一领域，通过设计专门的奖励函数，让代码生成模型（Coder）与单元测试模型（Tester）在交互中相互促进：Coder根据Tester的反馈修正代码缺陷，而Tester则通过捕捉Coder的错误持续优化测试用例生成策略。这种闭环学习模式摆脱了对人工标注数据的依赖，形成了自驱动的能力进化循环。

实验验证表明，基于Qwen2.5-Instruct优化的ReasonFlux-Coder模型在代码生成准确率上提升5.3%，Best-of-N准确率提升9%，显著超越同规模竞品。特别值得注意的是，4B参数的轻量级模型在单元测试生成任务中保持64.8%的推理效率同时，性能仍优于Qwen3-4B基线。研究还发现，该框架衍生的模型可作为强化学习的优质奖励信号，在下游任务如测试时扩展和智能体编程中带来8.1%的性能增益。

**最后一句**  
这种自监督的协同进化范式为AI编程系统的持续优化开辟了新路径，其核心思想可推广至更复杂的软件工程自动化场景。