# 250612_XAI-Units: Benchmarking Explainability Methods with Unit Tests

---
**论文信息**

- **标题**: XAI-Units: Benchmarking Explainability Methods with Unit Tests
- **arXiv ID**: 2506.01059
- **作者**: Authors:Jun Rui Lee, Sadegh Emami, Michael David Hollins, Timothy C. H. Wong, Carlos Ignacio Villalobos Sánchez, Francesca Toni, Dekai Zhang, Adam Dejl
- **发表日期**: 2025-06-01T15:58:27+00:00
- **论文链接**: [2506.01059](https://arxiv.org/abs/2506.01059)
- **总结生成时间**: 2025-06-12 22:57:52

---

**一句话概要**  
作者提出XAI-Units基准测试框架，通过构建已知内部机制的合成模型与数据集，系统评估不同特征归因方法在解释模型行为时的可靠性与适用性。

**主体**  
当前可解释AI领域面临的核心挑战在于，不同特征归因方法对同一模型给出的重要性评分常存在分歧，而缺乏真实基准或模型内部知识使得难以判断哪种方法更可信。这种评估困境阻碍了可解释方法在实际应用中的有效选择。作者指出，现有评估方式往往依赖主观人工判断或特定案例验证，缺乏针对模型基础推理单元（如特征交互、抵消效应等）的系统化测试标准。

为解决这一问题，研究团队借鉴软件工程中的单元测试思想，开发了开源的XAI-Units框架。该框架通过程序化生成具有明确内部逻辑的合成模型（如包含预设特征交互的多项式模型）及配套数据集，构建可验证的"标准答案"场景。例如，设计包含非线性跳跃输出的模型来测试归因方法对不连续行为的捕捉能力。框架内置多种评估指标，可量化比较不同方法在原子级模型行为上的表现差异，从而揭示其优势与局限。

实验验证表明，XAI-Units能有效识别主流特征归因方法在不同场景下的性能边界。例如，某些基于梯度的方法在特征抵消情境中会产生误导性解释，而扰动类方法对交互特征的检测更稳定。通过将7种典型方法在数十种预设模型行为上的测试结果可视化，研究不仅提供了方法选择的客观依据，更暴露出当前可解释技术普遍存在的理论缺陷。这种基于合成数据的基准测试范式，为建立可解释AI的标准化评估体系奠定了基础。

**最后一句**  
该工作通过引入可复现的单元化测试方法，为未来开发更鲁棒的可解释技术提供了可量化的改进方向，同时推动了XAI领域从经验驱动到理论驱动的范式转变。