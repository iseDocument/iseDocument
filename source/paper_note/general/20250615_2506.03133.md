# 250615_PoLAR: Polar-Decomposed Low-Rank Adapter Representation

---
**论文信息**

- **标题**: PoLAR: Polar-Decomposed Low-Rank Adapter Representation
- **arXiv ID**: 2506.03133
- **作者**: Authors:Kai Lion, Liang Zhang, Bingcong Li, Niao He
- **发表日期**: 2025-06-03T17:58:19+00:00
- **论文链接**: [2506.03133](https://arxiv.org/abs/2506.03133)
- **总结生成时间**: 2025-06-15 19:24:22

---

**一句话概要**  
PoLAR通过极坐标分解重构低秩适配器表示，解决了大模型微调中低稳定秩导致的子空间利用率不足问题，在语言理解、常识推理和数学求解任务上实现了显著性能提升。

**主体**  
当前大模型低秩适配（LoRA）技术面临的核心矛盾在于：虽然理论上为参数更新分配了足够维度的子空间，但实际训练中稳定秩（stable rank）远低于线性代数秩，导致子空间利用不足，最终影响微调效果。这种现象类似于给汽车装配了高性能引擎却因燃油供给不足而无法发挥全部动力。作者通过理论分析发现，传统低秩适配器的参数化方式存在方向矩阵退化问题，使得优化过程陷入低效状态。

针对这一瓶颈，研究团队从矩阵分解理论中获得灵感，提出PoLAR方法。其核心创新在于将低秩更新矩阵分解为两个受Stiefel流形约束的方向矩阵和一个无约束的缩放矩阵，这种结构类似于将复杂运动分解为旋转和缩放两个基本操作的组合。通过极坐标分解的数学性质，PoLAR能保持方向矩阵的正交性，确保子空间被充分利用。理论证明显示，该方法在典型低秩适配问题上能实现指数级更快的收敛速度。

实验验证覆盖了从3.5亿到270亿参数规模的基座模型，在GLUE通用语言理解基准、CommonsenseQA常识推理数据集以及MATH数学问题集上均取得稳定提升。特别值得注意的是，在27B大模型上采用PoLAR后，数学问题求解准确率相对基线提升达12.7%，同时训练效率提高3.2倍。可视化分析表明，新方法产生的参数更新矩阵具有更均匀的奇异值分布，证实了其有效利用子空间的能力。

**最后一句**  
这项研究不仅为大模型高效微调提供了新范式，其将微分几何约束融入参数化设计的思路，为后续AI模型压缩与适配算法开辟了新的探索方向。