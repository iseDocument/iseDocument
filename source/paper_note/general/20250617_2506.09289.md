# 250617_UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench

---
**论文信息**

- **标题**: UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench
- **arXiv ID**: 2506.09289
- **作者**: Authors:Boxi Yu, Yuxuan Zhu, Pinjia He, Daniel Kang
- **发表日期**: 2025-06-10T22:56:49+00:00
- **论文链接**: [2506.09289](https://arxiv.org/abs/2506.09289)
- **总结生成时间**: 2025-06-17 19:26:49

---

**一句话概要**  
研究揭示了当前代码生成评估基准SWE-Bench中测试用例不足导致误判的问题，提出基于大语言模型的UTBoost框架增强测试覆盖，显著修正了原有评估结果中的错误。

**主体**  
随着大语言模型驱动的代码生成代理兴起，SWE-Bench作为主流评估基准依赖GitHub问题与拉取请求中的测试用例，但其人工编写的测试往往存在覆盖不足的缺陷。作者发现，这种缺陷会导致生成的代码补丁即使未真正解决问题也能通过测试，使得评估结果出现系统性偏差。这一现象暴露出当前基准在验证功能正确性方面的关键短板，可能误导对模型能力的判断。

为解决该问题，研究团队设计了两阶段方案：首先开发UTGenerator工具，利用大语言模型自动分析代码库依赖关系并生成补充测试用例；进而构建UTBoost框架，通过动态增强测试集来严格验证补丁有效性。该方法创新性地将大语言模型双向应用于测试生成与补丁验证，形成闭环评估体系。实验选取Python项目作为验证场景，框架不仅识别出36个存在测试缺陷的任务实例，更检测到345个被原基准错误标记为通过的无效补丁。

验证结果显示，测试增强使SWE-Bench Lite和Verified两个子集的排行榜发生显著变化，分别有40.9%和24.4%的条目被修正，导致18项和11项排名变动。这些数据证实现有评估体系存在大量"假阳性"结果，而UTBoost能有效提升判别严格性。尤其值得注意的是，补充测试用例暴露出生成补丁在边界条件、异常处理等方面的深层缺陷，为改进代码生成模型提供了明确方向。

**最后一句**  
这项工作不仅为代码生成评估建立了更可靠的基准框架，其"以模型验证模型"的思路也为软件工程领域的自动化测试开辟了新路径。