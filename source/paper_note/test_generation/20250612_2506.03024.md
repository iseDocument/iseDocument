# 250612_GenFair: Systematic Test Generation for Fairness Fault Detection in Large Language Models

---
**论文信息**

- **标题**: GenFair: Systematic Test Generation for Fairness Fault Detection in Large Language Models
- **arXiv ID**: 2506.03024
- **作者**: Authors:Madhusudan Srinivasan, Jubril Abdel
- **发表日期**: 2025-06-03T16:00:30+00:00
- **论文链接**: [2506.03024](https://arxiv.org/abs/2506.03024)
- **总结生成时间**: 2025-06-12 17:23:10

---

**一句话概要**  
GenFair通过等价划分、变异算子和边界值分析系统生成多样化测试用例，结合蜕变关系检测大语言模型中传统方法难以捕捉的交叉性公平缺陷。

**主体**  
随着大语言模型在关键领域的广泛应用，其训练数据中隐含的偏见可能导致公平性风险，尤其是涉及多重人口特征交叉的复杂偏见场景。现有基于模板或语法规则的测试方法（如CheckList和ASTRAEA）存在测试多样性不足、对交叉性偏差敏感度低等缺陷，难以全面识别模型中的公平性漏洞。作者指出这些方法生成的测试用例往往过于同质化，且缺乏对现实语言复杂性的模拟，导致对深层偏见的检测能力有限。

为解决上述问题，研究团队提出GenFair框架，其核心创新在于将软件测试中的经典技术（等价划分、变异算子、边界值分析）与蜕变测试相结合。该方法首先生成具有语言多样性和现实性的源测试用例，覆盖不同人口特征的组合场景；随后通过预定义的蜕变关系自动衍生出语义关联的后续用例，并比较模型对源用例和后续用例的响应差异。特别地，GenFair采用基于语调的量化指标来捕捉模型输出中微妙的偏见倾向，例如对相同职业描述因性别或种族不同而产生的态度差异。这种设计使得框架能够自动发现传统人工规则难以定义的复杂公平性违规模式。

实验部分在GPT-4.0和LLaMA-3.0上的验证显示，GenFair的故障检测率分别达到0.73和0.69，显著超过模板方法（0.54/0.51）和ASTRAEA（0.39/0.36）。定量分析表明其生成的测试用例在句法多样性（10.06）和语义多样性（76.68）指标上均优于基线，同时保持较高的语义连贯性（0.7043）。可视化案例进一步证实，该方法能有效识别如"女性程序员应更友善"等隐含的交叉性偏见，这些案例在人工审查中被确认为真实存在的模型缺陷。

**最后一句**  
该框架为自动化公平性测试提供了可扩展的方案，其结合传统软件工程与AI伦理的跨学科思路，为构建更公平的语言模型开辟了新路径。