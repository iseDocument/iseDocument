# 250613_Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems

---
**论文信息**

- **标题**: Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems
- **arXiv ID**: 2506.06821
- **作者**: Authors:Yuhan Cao, Zian Chen, Kun Quan, Ziliang Zhang, Yu Wang, Xiaoning Dong, Yeqi Feng, Guanzhong He, Jingcheng Huang, Jianhao Li, Yixuan Tan, Jiafu Tang, Yilin Tang, Junlei Wu, Qianyu Xiao, Can Zheng, Shouchen Zhou, Yuxiang Zhu, Yiming Huang, Tian Xie, Tianxing He
- **发表日期**: 2025-06-07T14:53:03+00:00
- **论文链接**: [2506.06821](https://arxiv.org/abs/2506.06821)
- **总结生成时间**: 2025-06-13 15:03:02

---

**一句话概要**  
研究探讨了大语言模型（LLMs）在竞赛级编程问题中生成可靠测试用例生成器的能力，揭示了其在生成针对性测试用例方面的局限性，并提出通过高质量数据集提升模型性能的方法。

**主体**  
尽管大语言模型在代码生成任务中表现出色，但其在代码调试和测试用例生成领域的潜力尚未充分挖掘。作者聚焦于竞赛级编程问题，提出TCGBench基准测试，旨在评估LLMs生成有效测试用例生成器的能力，尤其是能否生成针对人类编写代码中缺陷的测试用例。研究发现，当前最先进的LLMs虽然能够生成基本有效的测试用例生成器，但在生成针对性测试用例以暴露代码缺陷方面表现欠佳，甚至高级推理模型如o3-mini也显著落后于人类水平。

为解决这一问题，作者构建了一个高质量的人工标注数据集，包含生成针对性测试用例生成器的指令。实验表明，无论是通过提示工程还是微调，该数据集都能显著提升LLMs在生成针对性测试用例方面的性能。这一发现不仅验证了数据驱动方法在提升模型能力方面的有效性，也为未来研究提供了宝贵的资源。

**最后一句**  
该工作为LLMs在软件工程测试领域的应用开辟了新方向，同时强调了高质量数据在提升模型针对性任务表现中的关键作用。