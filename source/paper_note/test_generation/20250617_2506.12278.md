# 250617_Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure

---
**论文信息**

- **标题**: Can LLMs Generate High-Quality Test Cases for Algorithm Problems? TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure
- **arXiv ID**: 2506.12278
- **作者**: Authors:Zheyuan Yang, Zexi Kuang, Xue Xia, Yilun Zhao
- **发表日期**: 2025-06-13T23:56:17+00:00
- **论文链接**: [2506.12278](https://arxiv.org/abs/2506.12278)
- **总结生成时间**: 2025-06-17 19:26:49

---

**一句话概要**  
研究通过构建TestCase-Eval基准系统评估了19种大语言模型生成算法测试用例的能力，揭示了其在故障覆盖率和针对性错误暴露方面的表现差异。

**主体**  
随着大语言模型在代码生成领域的广泛应用，一个关键问题逐渐浮现：这些模型能否生成高质量的算法测试用例？传统测试用例生成依赖人工设计，而作者发现当前缺乏系统性评估LLM在此任务中能力的标准。为此，研究团队从Codeforces平台收集了500个算法问题和10万个人工编写的解决方案，构建了TestCase-Eval评估框架，重点考察两个核心维度——故障覆盖率（衡量测试集探测多样化输入场景的能力）和故障暴露（评估模型针对特定错误代码生成触发用例的精准性）。

为解决评估难题，作者设计了分层抽样策略构建基准数据集，确保覆盖不同难度和类型的算法问题。在方法层面，研究创新性地将测试用例质量量化为两个可计算的指标：通过变异测试技术模拟潜在代码缺陷来评估故障覆盖率，同时要求模型针对已知错误实现逆向生成暴露用例以检验故障暴露能力。实验采用控制变量法，对19个开源和商业LLM进行横向对比，包括GPT-4、Claude等主流模型。结果显示，顶级商业模型在故障覆盖率上接近人类水平的85%，但在针对性错误暴露任务中，所有模型表现均下降约30%，暴露出对代码缺陷逻辑的理解局限。

**最后一句**  
这项研究为自动化测试用例生成建立了可量化的评估范式，其揭示的模型能力边界将推动未来面向缺陷感知的代码生成模型优化。