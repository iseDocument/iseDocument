# 250615_Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems

---
**论文信息**

- **标题**: Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems
- **arXiv ID**: 2506.06821
- **作者**: Authors:Yuhan Cao, Zian Chen, Kun Quan, Ziliang Zhang, Yu Wang, Xiaoning Dong, Yeqi Feng, Guanzhong He, Jingcheng Huang, Jianhao Li, Yixuan Tan, Jiafu Tang, Yilin Tang, Junlei Wu, Qianyu Xiao, Can Zheng, Shouchen Zhou, Yuxiang Zhu, Yiming Huang, Tian Xie, Tianxing He
- **发表日期**: 2025-06-07T14:53:03+00:00
- **论文链接**: [2506.06821](https://arxiv.org/abs/2506.06821)
- **总结生成时间**: 2025-06-15 19:24:22

---

**一句话概要**  
研究探讨了大语言模型（LLMs）能否为竞赛级编程问题生成可靠的测试用例生成器，揭示了其在生成针对性测试用例方面的局限性，并提出通过高质量数据集提升模型性能的方法。

**主体**  
尽管大语言模型在代码生成任务中表现出色，但其在代码检查与调试中的应用潜力尚未充分挖掘，尤其是能否通过生成测试用例来暴露程序缺陷。作者聚焦竞赛级编程场景，提出TCGBench基准测试框架，包含两项核心任务：一是评估模型生成有效测试用例生成器的能力，二是测试模型能否生成针对人类编写代码中缺陷的定向测试用例。实验发现，当前最先进的LLMs虽能生成基本有效的测试用例生成器，但在生成针对性用例时表现欠佳，即使如o3-mini等高级推理模型也显著落后于人类水平。

为解决这一局限，作者构建了一个高质量的人工标注数据集，专门用于指导生成针对性测试用例生成器。分析表明，无论是通过提示工程还是微调，该数据集均能显著提升LLMs的性能。例如，在定向测试用例生成任务中，模型借助数据集后成功率提升了约30%，但仍未达到人类专家的水平。这一结果突显了LLMs在复杂逻辑推理任务中的不足，同时也为改进方向提供了实证依据。

**最后一句**  
该研究不仅为LLMs在软件测试领域的应用划定了能力边界，其构建的数据集和评估框架也为未来开发更精准的代码调试辅助工具奠定了基础。