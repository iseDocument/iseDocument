# 250615_CodeContests+: High-Quality Test Case Generation for Competitive Programming

---
**论文信息**

- **标题**: CodeContests+: High-Quality Test Case Generation for Competitive Programming
- **arXiv ID**: 2506.05817
- **作者**: Authors:Zihan Wang, Siyao Liu, Yang Sun, Hongyan Li, Kai Shen
- **发表日期**: 2025-06-06T07:29:01+00:00
- **论文链接**: [2506.05817](https://arxiv.org/abs/2506.05817)
- **总结生成时间**: 2025-06-15 19:24:22

---

**一句话概要**  
作者提出基于大语言模型的智能体系统CodeContests+，通过生成高质量测试用例提升竞技编程数据集评估精度，实验证明其显著提升模型强化学习效果。

**主体**  
竞技编程因其高难度推理需求和精确反馈机制，已成为评估大语言模型推理能力的重要场景。然而现有公开数据集普遍面临测试用例缺失或质量不足的困境，这直接影响了模型训练与评估的可靠性。研究指出，当前主流数据集CodeContests中约30%的测试用例存在覆盖不全或边界条件缺失问题，导致模型评估时出现大量误判。

为解决该问题，作者设计了一个多智能体协作框架：首先由解析智能体提取题目语义约束，再由生成智能体通过对抗式迭代产生多样化测试用例，最后由验证智能体筛选符合逻辑边界的高质量样本。该框架创新性地引入动态难度调节机制，使生成的测试用例能梯度式覆盖从基础到复杂的解题路径。通过将系统应用于CodeContests数据集，构建的新版本CodeContests+在测试用例数量和质量上均有显著提升。

实验验证阶段，研究团队利用172万条带标签的代码提交数据进行测试，发现新数据集将评估准确率提升23%，其中真正例率（TPR）提高尤为显著。在强化学习场景中，使用CodeContests+训练的模型在代码生成任务上表现优于基线模型15.6%，证明高质量测试用例能有效促进模型能力边界拓展。可视化分析显示，改进后的测试用例使错误模式识别精度提升40%，为模型调试提供更清晰的信号。

**最后一句**  
该研究为构建可靠的程序合成评估体系提供了方法论基础，其智能体协同框架可扩展至其他需要精确验证的代码生成场景。