# 250615_GenFair: Systematic Test Generation for Fairness Fault Detection in Large Language Models

---
**论文信息**

- **标题**: GenFair: Systematic Test Generation for Fairness Fault Detection in Large Language Models
- **arXiv ID**: 2506.03024
- **作者**: Authors:Madhusudan Srinivasan, Jubril Abdel
- **发表日期**: 2025-06-03T16:00:30+00:00
- **论文链接**: [2506.03024](https://arxiv.org/abs/2506.03024)
- **总结生成时间**: 2025-06-15 19:24:22

---

**一句话概要**  
GenFair通过等价划分、变异算子和边界值分析系统生成多样性测试用例，结合蜕变关系检测大语言模型中传统方法难以捕捉的交叉性公平缺陷。

**主体**  
随着大语言模型在关键领域的广泛应用，其训练数据中隐含的偏见导致公平性问题日益凸显，尤其当不同人口统计特征交叉作用时，传统基于模板或语法规则的测试方法（如CheckList和ASTRAEA）因测试多样性不足和敏感性有限，难以有效识别这类复杂偏差。作者指出，现有方法生成的测试用例往往缺乏语言多样性和现实场景代表性，且对交叉性偏见的检测能力较弱。

为解决这一挑战，研究团队提出GenFair框架，其核心创新在于将软件测试中的等价划分、变异操作和边界值分析技术迁移至公平性测试领域。该方法首先生成具有语言多样性和人口统计交叉特征的源测试用例，随后通过蜕变关系自动衍生出语义关联的后续用例，最终通过对比模型对源用例和后续用例的响应语气差异来识别公平性违规。这种设计既保证了测试用例的丰富性，又能捕捉模型对不同人口属性组合的微妙区别对待。

实验环节在GPT-4.0和LLaMA-3.0上的验证表明，GenFair的故障检测率分别达到0.73和0.69，显著优于模板方法（0.54/0.51）和ASTRAEA（0.39/0.36）。其测试用例在句法多样性（10.06）和语义多样性（76.68）指标上均表现最优，同时保持高连贯性（句法连贯性291.32，语义连贯性0.7043）。可视化分析进一步证实，该方法能有效暴露传统测试覆盖盲区中的交叉性偏见模式。

**最后一句**  
该框架为自动化公平性测试提供了可扩展的方案，其蜕变测试范式对未来开发更公平的AI系统具有方法论启示。