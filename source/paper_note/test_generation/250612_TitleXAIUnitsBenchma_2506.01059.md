---
**论文信息**

- **标题**: Title:XAI-Units: Benchmarking Explainability Methods with Unit Tests
- **arXiv ID**: 2506.01059
- **作者**: Authors:Jun Rui Lee, Sadegh Emami, Michael David Hollins, Timothy C. H. Wong, Carlos Ignacio Villalobos Sánchez, Francesca Toni, Dekai Zhang, Adam Dejl
- **发表日期**: 2025-06-01T15:58:27+00:00
- **论文链接**: [2506.01059](https://arxiv.org/abs/2506.01059)
- **总结生成时间**: 2025-06-12 22:17:32

---

**一句话概要**  
作者提出XAI-Units基准测试框架，通过构建已知内部机制的合成模型与数据集，系统评估不同特征归因方法在解释模型行为时的可靠性与适用性。

**主体**  
当前可解释人工智能领域面临的核心挑战在于，不同特征归因方法对同一模型给出的重要性评分常存在分歧，而缺乏真实基准或模型内部知识使得难以判断哪种方法更适合特定场景。这种不确定性严重制约了XAI技术在关键决策场景中的可信应用。为解决该问题，研究团队借鉴软件工程中的单元测试思想，开发了开源的XAI-Units基准框架。其创新性在于通过程序化生成具有明确内部逻辑的合成模型（如包含特征交互、抵消效应或非连续输出等行为），并配套构建对应数据集，从而建立可验证的"理想归因分数"标准。

该框架的核心设计包含两个关键维度：一是覆盖多种原子级模型推理模式（如图2展示的乘法特征交互测试案例），二是内置包括归因一致性、鲁棒性在内的多维评估指标。通过将复杂模型行为解耦为独立测试单元，研究者能够精确分析不同特征归因方法（如SHAP、LIME等）在特定推理模式下的表现差异。实验部分通过7组对比测试揭示，现有方法在特征抵消等复杂场景中普遍存在解释偏差，而基于梯度的方法在非线性关系建模时表现更稳定。

**启示**  
这项研究为XAI领域建立了首个系统化的单元测试范式，其模块化设计不仅推动了归因方法的客观比较，更为开发适应特定推理需求的可解释工具提供了方法论基础。