---
**论文信息**

- **标题**: Title:XAI-Units: Benchmarking Explainability Methods with Unit Tests
- **arXiv ID**: 2506.01059
- **作者**: Authors:Jun Rui Lee, Sadegh Emami, Michael David Hollins, Timothy C. H. Wong, Carlos Ignacio Villalobos Sánchez, Francesca Toni, Dekai Zhang, Adam Dejl
- **发表日期**: 2025-06-01T15:58:27+00:00
- **论文链接**: [2506.01059](https://arxiv.org/abs/2506.01059)
- **总结生成时间**: 2025-06-12 22:37:01

---

**一句话概要**  
作者提出XAI-Units基准测试框架，通过构建已知内部机制的合成模型与数据集，系统评估不同特征归因方法在解释模型行为时的可靠性与适用性。

**主体**  
当前可解释人工智能领域面临的核心挑战在于，不同特征归因方法对同一模型给出的重要性评分常存在分歧，而缺乏真实基准使得难以判断哪种方法更适合特定场景。这种困境源于黑盒模型内部机制的不透明性，以及现实数据中复杂特征交互带来的评估模糊性。研究指出，现有评估方式过度依赖主观判断或特定用例，缺乏针对模型基础推理单元（如特征抵消、不连续输出等）的系统化测试标准。

为解决这一问题，作者借鉴软件工程中单元测试的思想，开发了开源的XAI-Units框架。该框架通过程序化生成具有明确内部逻辑的合成模型（如预设特征交互规则的线性模型或分段函数），配套构建对应数据集，形成可验证的"标准答案"环境。每个测试单元聚焦单一模型行为模式，例如通过设计输入特征间的乘法关系来检验归因方法对交互效应的捕捉能力。框架内置覆盖性、一致性等量化指标，支持对归因方法进行原子级能力拆解，如同为XAI方法建立了一套标准化体检项目。

实验验证表明，XAI-Units能有效揭示不同归因方法的特性边界。例如在测试特征抵消场景时，某些基于梯度的归因方法会错误分配重要性分数，而基于扰动的方法则表现出更稳定的性能。通过七个核心测试单元和数十种模型组合，研究不仅验证了框架的诊断能力，还发现了现有方法在处理非线性跳跃等复杂行为时的普遍缺陷。这种细粒度评估为方法选择提供了客观依据，比如显示SHAP在交互场景优于LIME，但在连续性假设失效时两者均可能失效。

**最后一句**  
这项研究通过建立可扩展的基准测试生态，为开发更鲁棒的解释方法提供了验证基础设施，同时启示未来工作需关注归因方法与模型认知结构的适应性匹配。