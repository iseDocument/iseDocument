# 250617_AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine

---
**论文信息**

- **标题**: AutoGEEval++: A Multi-Level and Multi-Geospatial-Modality Automated Evaluation Framework for Large Language Models in Geospatial Code Generation on Google Earth Engine
- **arXiv ID**: 2506.10365
- **作者**: Authors:Shuyang Hou, Zhangxiao Shen, Huayi Wu, Haoyue Jiao, Ziqi Liu, Lutong Xie, Chang Liu, Jianyuan Liang, Yaxian Qing, Xiaopu Zhang, Dehua Peng, Zhipeng Gui, Xuefeng Guan
- **发表日期**: 2025-06-12T05:42:37+00:00
- **论文链接**: [2506.10365](https://arxiv.org/abs/2506.10365)
- **总结生成时间**: 2025-06-17 19:26:49

---

**一句话概要**  
AutoGEEval++通过构建多模态、多任务复杂度的自动化评估框架，首次为Google Earth Engine地理空间代码生成的LLM建立了标准化评测基准，揭示了不同模型在垂直领域的性能差异。

**主体**  
随着人工智能与地理空间分析的深度融合，地理空间代码生成成为关键研究方向，但缺乏标准化评估工具的问题长期存在。作者指出，现有评测方法难以覆盖Google Earth Engine（GEE）平台上多样化的数据类型（如遥感影像、矢量数据）和任务复杂度（从基础单元操作到主题分析），导致模型性能评估碎片化。这一空白阻碍了领域专用大语言模型的优化与横向比较。

为解决该问题，研究团队在AutoGEEval基础上提出增强框架AutoGEEval++，其核心创新在于构建了支持26种数据类型、6365个测试用例的基准数据集AutoGEEval++-Bench，涵盖单元测试、组合测试和主题测试三类任务。框架通过Python API集成GEE执行环境，采用提交程序与裁判模块组成的端到端流水线，实现从代码生成到执行验证的全流程自动化。评测维度突破传统准确率指标，引入资源占用、运行时效率、错误类型分析等多元指标，特别设计了控制幻觉与边界测试的机制。例如，通过监测内存溢出和API误用等12类错误模式，系统能精准定位模型在空间投影转换、波段计算等专业场景的缺陷。

实验验证环节，作者对24种前沿LLM（包括通用型、推理增强型、代码专用型和地学领域模型）展开横向评测。结果显示，不同模型在任务类型和部署环境下的表现存在显著差异：代码专用模型在单元测试中准确率超80%，但在主题测试中下降至42%；地学领域模型虽稳定性更优，却受限于训练数据时效性。这些发现不仅证实了框架的评估效能，还揭示了领域知识注入与代码生成能力的非线性关系。框架的扩展性在新增冰川监测等5类地学任务测试中得到进一步验证。

**最后一句**  
该研究为垂直领域代码生成评估提供了方法论范式，其模块化设计思路可延伸至气象、海洋等其他地理空间子领域，推动AI与专业科学的交叉创新。