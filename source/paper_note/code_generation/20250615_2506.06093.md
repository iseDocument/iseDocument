# 250615_Reinforcing Code Generation: Improving Text-to-SQL with Execution-Based Learning

---
**论文信息**

- **标题**: Reinforcing Code Generation: Improving Text-to-SQL with Execution-Based Learning
- **arXiv ID**: 2506.06093
- **作者**: Authors:Atharv Kulkarni, Vivek Srikumar
- **发表日期**: 2025-06-06T13:52:41+00:00
- **论文链接**: [2506.06093](https://arxiv.org/abs/2506.06093)
- **总结生成时间**: 2025-06-15 19:24:22

---

**一句话概要**  
作者提出通过强化学习框架利用数据库执行反馈来优化大语言模型的文本到SQL生成能力，显著提升了查询准确率并接近了更大规模模型的性能。

**主体**  
当前大语言模型在文本到SQL任务中面临的核心挑战在于传统监督微调依赖大量文本-代码配对数据，而这类数据往往获取成本高昂且覆盖场景有限。针对这一问题，作者创新性地将SQL生成建模为强化学习任务，让模型通过与数据库引擎的交互获得即时执行反馈——查询失败时获得负向奖励，返回正确结果时获得正向奖励。这种基于执行结果的反馈机制绕过了对标注SQL语句的依赖，仅需弱监督的问题-答案对即可实现模型优化。

方法上，研究采用Group Relative Policy Optimization（GRPO）框架整合奖励信号，其核心思想是通过分组策略优化平衡探索与利用。实验在表格推理基准测试中验证，结果显示强化学习调优使模型生成的SQL准确率从31.49%跃升至49.83%，错误率降低超过10个百分点。值得注意的是，优化后的模型性能已接近参数量大得多的SQLCoder-70B模型，这证实了执行反馈对提升符号推理能力的有效性。

**最后一句**  
该工作为降低领域特定代码生成的标注依赖提供了新范式，其强化学习框架可扩展至其他需要精确符号操作的AI编程辅助场景。