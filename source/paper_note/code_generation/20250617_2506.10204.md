# 250617_Prompt Variability Effects On LLM Code Generation

---
**论文信息**

- **标题**: Prompt Variability Effects On LLM Code Generation
- **arXiv ID**: 2506.10204
- **作者**: Authors:Andrei Paleyes, Radzim Sendyka, Diana Robinson, Christian Cabrera, Neil D. Lawrence
- **发表日期**: 2025-06-11T21:43:48+00:00
- **论文链接**: [2506.10204](https://arxiv.org/abs/2506.10204)
- **总结生成时间**: 2025-06-17 19:26:49

---

**一句话概要**  
研究揭示了大型语言模型（LLM）在代码生成任务中对提示词变动的敏感性，并提出了一套与任务和模型无关的评估框架来量化这种影响。

**主体**  
当前LLM在代码生成领域的广泛应用虽然降低了编程门槛，但其输出质量高度依赖用户输入的提示词质量，特别是用户的技术背景差异会导致生成代码的功能性和质量出现显著波动。这种敏感性尚未被系统化测量，使得开发者难以预测不同提示策略的实际效果。作者指出，现有评估方法往往局限于特定编程任务或模型，缺乏普适性标准来衡量提示词变动对生成结果的影响。

为解决这一问题，研究团队设计了两套创新评估方案：其一是基于合成数据的代码生成评估流程，通过控制变量法分析提示词微小变化对输出的影响；其二是人物角色驱动的系统评估法，模拟不同技术背景的用户（如初学者与资深工程师）提交提示词时LLM的响应差异。这两种方法均不依赖具体编程语言或模型架构，通过抽象化评估维度实现了跨场景适用性。实验部分通过7组可视化图表和1张汇总表证明，即使是同义改写或语气调整这类细微提示变化，也可能导致代码正确率出现20%以上的波动。

**最后一句**  
这项研究为提示工程提供了可量化的评估工具，其通用性框架将助力未来开发更稳定的代码生成系统，同时启示需要针对不同用户群体设计差异化的交互策略。